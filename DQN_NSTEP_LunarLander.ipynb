{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uhsipra/CMPUT655_FinalProject/blob/main/CMPUT655_DQN_NSTEP_Algo_LunarLander_Config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuHO2FF5M9jM"
      },
      "source": [
        "## Configurations for Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EY5cSezCj_8",
        "outputId": "e3e5b156-8538-4d7f-cafc-dcb816323af2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDZFH77CM9jO",
        "outputId": "c6ef3678-0c0e-42fb-ad42-f133cccc6690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package python-opengl\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 1s (6,101 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120899 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting PyVirtualDisplay==3.0\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: PyVirtualDisplay\n",
            "Successfully installed PyVirtualDisplay-3.0\n",
            "Collecting gymnasium==0.28.1\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (1.23.5)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.28.1)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from gymnasium[box2d])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1.post1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373077 sha256=5b655cdd9b5ac7d59aadaf57828997574bc232a4900060c44da28c0c564db272\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install -y python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install PyVirtualDisplay==3.0\n",
        "    !pip install gymnasium==0.28.1\n",
        "    !pip install swig\n",
        "    !pip install gymnasium[box2d]\n",
        "    from pyvirtualdisplay import Display\n",
        "\n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(400, 400))\n",
        "    dis.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4sSCWUzM9jQ"
      },
      "source": [
        "# 07. N-Step Learning\n",
        "\n",
        "[R. S. Sutton, \"Learning to predict by the methods of temporal differences.\" Machine learning, 3(1):9–44, 1988.](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)\n",
        "\n",
        "Q-learning accumulates a single reward and then uses the greedy action at the next step to bootstrap. Alternatively, forward-view multi-step targets can be used (Sutton 1988). We call it Truncated N-Step Return\n",
        "from a given state $S_t$. It is defined as,\n",
        "\n",
        "$$\n",
        "R^{(n)}_t = \\sum_{k=0}^{n-1} \\gamma_t^{(k)} R_{t+k+1}.\n",
        "$$\n",
        "\n",
        "A multi-step variant of DQN is then defined by minimizing the alternative loss,\n",
        "\n",
        "$$\n",
        "(R^{(n)}_t + \\gamma^{(n)}_t \\max_{a'} q_{\\theta}^{-}\n",
        "(S_{t+n}, a')\n",
        "- q_{\\theta}(S_t, A_t))^2.\n",
        "$$\n",
        "\n",
        "Multi-step targets with suitably tuned $n$ often lead to faster learning (Sutton and Barto 1998)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qxi1Lc0rM9jQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import deque\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tqdm import tqdm\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9HhwrqM9jR"
      },
      "source": [
        "## Replay buffer for N-step learning\n",
        "\n",
        "There are a little bit changes in Replay buffer for N-step learning. First, we use `deque` to store the most recent n-step transitions.\n",
        "\n",
        "```python\n",
        "    self.n_step_buffer = deque(maxlen=n_step)\n",
        "```\n",
        "\n",
        "You can see it doesn't actually store a transition in the buffer, unless `n_step_buffer` is full.\n",
        "\n",
        "```\n",
        "    # in store method\n",
        "    if len(self.n_step_buffer) < self.n_step:\n",
        "        return ()\n",
        "```\n",
        "\n",
        "When the length of `n_step_buffer` becomes equal to N, it eventually stores the N-step transition, which is calculated by `_get_n_step_info` method.\n",
        "\n",
        "(Please see *01.dqn.ipynb* for detailed description of the basic replay buffer.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mz7raz_uM9jR"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        size: int,\n",
        "        batch_size: int = 32,\n",
        "        n_step: int = 3,\n",
        "        gamma: float = 0.99,\n",
        "    ):\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "        # for N-step Learning\n",
        "        self.n_step_buffer = deque(maxlen=n_step)\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray,\n",
        "        rew: float,\n",
        "        next_obs: np.ndarray,\n",
        "        done: bool\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
        "        transition = (obs, act, rew, next_obs, done)\n",
        "        self.n_step_buffer.append(transition)\n",
        "\n",
        "        # single step transition is not ready\n",
        "        if len(self.n_step_buffer) < self.n_step:\n",
        "            return ()\n",
        "\n",
        "        # make a n-step transition\n",
        "        rew, next_obs, done = self._get_n_step_info(\n",
        "            self.n_step_buffer, self.gamma\n",
        "        )\n",
        "        obs, act = self.n_step_buffer[0][:2]\n",
        "\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "        return self.n_step_buffer[0]\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        indices = np.random.choice(\n",
        "            self.size, size=self.batch_size, replace=False\n",
        "        )\n",
        "\n",
        "        return dict(\n",
        "            obs=self.obs_buf[indices],\n",
        "            next_obs=self.next_obs_buf[indices],\n",
        "            acts=self.acts_buf[indices],\n",
        "            rews=self.rews_buf[indices],\n",
        "            done=self.done_buf[indices],\n",
        "            # for N-step Learning\n",
        "            indices=indices,\n",
        "        )\n",
        "\n",
        "    def sample_batch_from_idxs(\n",
        "        self, indices: np.ndarray\n",
        "    ) -> Dict[str, np.ndarray]:\n",
        "        # for N-step Learning\n",
        "        return dict(\n",
        "            obs=self.obs_buf[indices],\n",
        "            next_obs=self.next_obs_buf[indices],\n",
        "            acts=self.acts_buf[indices],\n",
        "            rews=self.rews_buf[indices],\n",
        "            done=self.done_buf[indices],\n",
        "        )\n",
        "\n",
        "    def _get_n_step_info(\n",
        "        self, n_step_buffer: Deque, gamma: float\n",
        "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
        "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
        "        # info of the last transition\n",
        "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
        "\n",
        "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
        "            r, n_o, d = transition[-3:]\n",
        "\n",
        "            rew = r + gamma * rew * (1 - d)\n",
        "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
        "\n",
        "        return rew, next_obs, done\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ele8VjNNM9jS"
      },
      "source": [
        "## Network\n",
        "\n",
        "We are going to use a simple network architecture with three fully connected layers and two non-linearity functions (ReLU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "86r-7PPXM9jS"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SgMpndpM9jS"
      },
      "source": [
        "## DQN Agent + N-step learning Agent\n",
        "\n",
        "Here is a summary of DQNAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "| ---              | ---                                                  |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|compute_dqn_loss  | return dqn loss.                                     |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|target_hard_update| hard update from the local model to the target model.|\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|plot              | plot the training progresses.                        |\n",
        "\n",
        "We use two buffers: `memory` and `memory_n` for 1-step transitions and n-step transitions respectively. It guarantees that any paired 1-step and n-step transitions have the same indices (See `step` method for more details). Due to the reason, we can sample pairs of transitions from the two buffers once we have indices for samples.\n",
        "\n",
        "```python\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        ...\n",
        "        samples = self.memory.sample_batch()\n",
        "        indices = samples[\"indices\"]\n",
        "        ...\n",
        "        \n",
        "        # N-step Learning loss\n",
        "        if self.use_n_step:\n",
        "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
        "            ...\n",
        "```\n",
        "\n",
        "One thing to note that  we are gonna combine 1-step loss and n-step loss so as to control high-variance / high-bias trade-off.\n",
        "\n",
        "(Search the comments with *N-step Leaning* to see any difference from DQN.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sr2WzfBwM9jT"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"DQN Agent interacting with environment.\n",
        "\n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        memory (ReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        epsilon (float): parameter for epsilon greedy policy\n",
        "        epsilon_decay (float): step size to decrease epsilon\n",
        "        max_epsilon (float): max value of epsilon\n",
        "        min_epsilon (float): min value of epsilon\n",
        "        target_update (int): period for target model's hard update\n",
        "        gamma (float): discount factor\n",
        "        dqn (Network): model to train and select actions\n",
        "        dqn_target (Network): target model to update\n",
        "        optimizer (torch.optim): optimizer for training dqn\n",
        "        transition (list): transition information including\n",
        "                           state, action, reward, next_state, done\n",
        "        use_n_step (bool): whether to use n_step memory\n",
        "        n_step (int): step number to calculate n-step td error\n",
        "        memory_n (ReplayBuffer): n-step replay buffer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        target_update: int,\n",
        "        epsilon_decay: float,\n",
        "        seed: int,\n",
        "        max_epsilon: float = 1.0,\n",
        "        min_epsilon: float = 0.1,\n",
        "        gamma: float = 0.99,\n",
        "        # N-step Learning\n",
        "        n_step: int = 3,\n",
        "    ):\n",
        "        \"\"\"Initialization.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): openAI Gym environment\n",
        "            memory_size (int): length of memory\n",
        "            batch_size (int): batch size for sampling\n",
        "            target_update (int): period for target model's hard update\n",
        "            epsilon_decay (float): step size to decrease epsilon\n",
        "            lr (float): learning rate\n",
        "            max_epsilon (float): max value of epsilon\n",
        "            min_epsilon (float): min value of epsilon\n",
        "            gamma (float): discount factor\n",
        "            n_step (int): step number to calculate n-step td error\n",
        "        \"\"\"\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "\n",
        "        self.env = env\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = max_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.seed = seed\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # memory for 1-step Learning\n",
        "        self.memory = ReplayBuffer(\n",
        "            obs_dim, memory_size, batch_size, n_step=1, gamma=gamma\n",
        "        )\n",
        "\n",
        "        # memory for N-step Learning\n",
        "        self.use_n_step = True if n_step > 1 else False\n",
        "        if self.use_n_step:\n",
        "            self.n_step = n_step\n",
        "            self.memory_n = ReplayBuffer(\n",
        "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n",
        "            )\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        self.device = \"cpu\"\n",
        "        #print(self.device)\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters(), lr = 1e-4)\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # epsilon greedy policy\n",
        "        if self.epsilon > np.random.random():\n",
        "            selected_action = self.env.action_space.sample()\n",
        "        else:\n",
        "            selected_action = self.dqn(\n",
        "                torch.FloatTensor(state).to(self.device)\n",
        "            ).argmax()\n",
        "            selected_action = selected_action.detach().cpu().numpy()\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition = [state, selected_action]\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action: np.ndarray, var) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if not self.is_test:\n",
        "            #sample reward from dist\n",
        "            if var > 0:\n",
        "                reward = Normal(reward, np.abs(reward)*var).sample().item()\n",
        "\n",
        "            self.transition += [reward, next_state, done]\n",
        "\n",
        "            # N-step transition\n",
        "            if self.use_n_step:\n",
        "                one_step_transition = self.memory_n.store(*self.transition)\n",
        "            # 1-step transition\n",
        "            else:\n",
        "                one_step_transition = self.transition\n",
        "\n",
        "            # add a single step transition\n",
        "            if one_step_transition:\n",
        "                self.memory.store(*one_step_transition)\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        samples = self.memory.sample_batch()\n",
        "        indices = samples[\"indices\"]\n",
        "        loss = self._compute_dqn_loss(samples, self.gamma)\n",
        "\n",
        "        # N-step Learning loss\n",
        "        # we are gonna combine 1-step loss and n-step loss so as to\n",
        "        # prevent high-variance.\n",
        "        if self.use_n_step:\n",
        "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
        "            gamma = self.gamma ** self.n_step\n",
        "            n_loss = self._compute_dqn_loss(samples, gamma)\n",
        "            loss += n_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, num_frames: int, var: float, test_interval: int = 200, csv_writer = None):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "\n",
        "        state, _ = self.env.reset(seed=self.seed)\n",
        "        update_cnt = 0\n",
        "        epsilons = []\n",
        "        losses = []\n",
        "        scores = []\n",
        "        score = 0\n",
        "        elen_list = []\n",
        "        elen = 0\n",
        "\n",
        "        for frame_idx in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action, var)\n",
        "            elen+=1\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state, _ = self.env.reset(seed=self.seed)\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory) >= self.batch_size:\n",
        "                loss = self.update_model()\n",
        "                losses.append(loss)\n",
        "                update_cnt += 1\n",
        "\n",
        "                # linearly decrease epsilon\n",
        "                self.epsilon = max(\n",
        "                    self.min_epsilon, self.epsilon - (\n",
        "                        self.max_epsilon - self.min_epsilon\n",
        "                    ) * self.epsilon_decay\n",
        "                )\n",
        "                epsilons.append(self.epsilon)\n",
        "\n",
        "                # if hard update is needed\n",
        "                if update_cnt % self.target_update == 0:\n",
        "                    self._target_hard_update()\n",
        "\n",
        "            if (frame_idx%test_interval==0):\n",
        "                score = self.test(\"videos/n_step_learning\")\n",
        "\n",
        "                # plotting\n",
        "                if csv_writer is None and frame_idx % test_interval == 0:\n",
        "                    self._plot(frame_idx, scores, losses, epsilons)\n",
        "\n",
        "                #else writing\n",
        "                elen_list.append(score)\n",
        "                state, _ = self.env.reset(seed=self.seed)\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "        if csv_writer is not None:\n",
        "            csv_writer.writerow([f\"{self.memory.max_size}/{var}\", *elen_list])\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def test(self, video_folder: str, num_eps = 3) -> None:\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "\n",
        "        # for recording a video\n",
        "        naive_env = self.env\n",
        "        #self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)\n",
        "\n",
        "        score = 0\n",
        "        eps = 0\n",
        "        while eps < num_eps:\n",
        "            eps += 1\n",
        "            done = False\n",
        "            state, _ = self.env.reset(seed=self.seed)\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                with torch.inference_mode():\n",
        "                    next_state, reward, done = self.step(action, 0)\n",
        "\n",
        "                state = next_state\n",
        "                score += reward\n",
        "        score/=num_eps\n",
        "        self.env.close()\n",
        "\n",
        "        # reset\n",
        "        self.env = naive_env\n",
        "        return score\n",
        "\n",
        "    def _compute_dqn_loss(\n",
        "        self,\n",
        "        samples: Dict[str, np.ndarray],\n",
        "        gamma: float\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Return dqn loss.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "\n",
        "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
        "        #       = r                       otherwise\n",
        "        curr_q_value = self.dqn(state).gather(1, action)\n",
        "        next_q_value = self.dqn_target(next_state).max(\n",
        "            dim=1, keepdim=True\n",
        "        )[0].detach()\n",
        "        mask = 1 - done\n",
        "        target = (reward + gamma * next_q_value * mask).to(self.device)\n",
        "\n",
        "        # calculate dqn loss\n",
        "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _target_hard_update(self):\n",
        "        \"\"\"Hard update: target <- local.\"\"\"\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "    def _plot(\n",
        "        self,\n",
        "        frame_idx: int,\n",
        "        scores: List[float],\n",
        "        losses: List[float],\n",
        "        epsilons: List[float],\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        plt.subplot(131)\n",
        "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
        "        plt.plot(scores)\n",
        "        plt.subplot(132)\n",
        "        plt.title('loss')\n",
        "        plt.plot(losses)\n",
        "        plt.subplot(133)\n",
        "        plt.title('epsilons')\n",
        "        plt.plot(epsilons)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG0erORJM9jT"
      },
      "source": [
        "## Environment\n",
        "\n",
        "You can see the [code](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py) and [configurations](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L91) of CartPole-v1 from Farama Gymnasium's repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yxg_1yBXM9jU"
      },
      "outputs": [],
      "source": [
        "# environment\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVlhzJj9M9jU"
      },
      "source": [
        "## Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c_4Bhh_rM9jU"
      },
      "outputs": [],
      "source": [
        "seed = 777\n",
        "\n",
        "def seed_torch(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.backends.cudnn.enabled:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "np.random.seed(seed)\n",
        "seed_torch(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZJ0re1bM9jU"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jHXcvVhCM9jU"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "#num_frames = 500\n",
        "#memory_size = 2000\n",
        "#batch_size = 32\n",
        "#target_update = 100\n",
        "#epsilon_decay = 1 / 2000\n",
        "\n",
        "# train\n",
        "#agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay, seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0eIX0T8M9jV"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hwSf5SnlM9jV"
      },
      "outputs": [],
      "source": [
        "#agent.train(num_frames, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FjFco6VM9jV"
      },
      "source": [
        "## Test\n",
        "\n",
        "Run the trained agent (1 episode)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZYsLjHUaM9jV"
      },
      "outputs": [],
      "source": [
        "#video_folder=\"videos/n_step_learning\"\n",
        "#agent.test(video_folder=video_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtL30Xx1M9jW"
      },
      "source": [
        "## Render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F-e26GG2M9jW"
      },
      "outputs": [],
      "source": [
        "#import base64\n",
        "#import glob\n",
        "#import io\n",
        "#import os\n",
        "\n",
        "#from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "# def ipython_show_video(path: str) -> None:\n",
        "#     \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "#     if not os.path.isfile(path):\n",
        "#         raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "#     video = io.open(path, \"r+b\").read()\n",
        "#     encoded = base64.b64encode(video)\n",
        "\n",
        "#     display(HTML(\n",
        "#         data=\"\"\"\n",
        "#         <video width=\"320\" height=\"240\" alt=\"test\" controls>\n",
        "#         <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "#         </video>\n",
        "#         \"\"\".format(encoded.decode(\"ascii\"))\n",
        "#     ))\n",
        "\n",
        "\n",
        "# def show_latest_video(video_folder: str) -> str:\n",
        "#     \"\"\"Show the most recently recorded video from video folder.\"\"\"\n",
        "#     list_of_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
        "#     latest_file = max(list_of_files, key=os.path.getctime)\n",
        "#     ipython_show_video(latest_file)\n",
        "#     return latest_file\n",
        "\n",
        "\n",
        "# latest_file = show_latest_video(video_folder=video_folder)\n",
        "# print(\"Played:\", latest_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hzmFUb3SvVU"
      },
      "source": [
        "## Run Trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je84pzKfSvVU",
        "outputId": "eafb0c95-3edc-4e0f-90bd-feead779c54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "num_frames = 200000\n",
        "num_trials = 5\n",
        "test_every = 200\n",
        "batch_size = 64\n",
        "env_name = \"LunarLander-v2\"\n",
        "variences = [0, 0.6, 1.2, 2]\n",
        "db_sizes = [10000, 30000, 50000, 80000, 100000]\n",
        "\n",
        "\n",
        "total_tests = num_trials * len(variences) * len(db_sizes)\n",
        "\n",
        "seed = 11\n",
        "\n",
        "csvfile = open(f'drive/MyDrive/{env_name}-seed{seed}-batch{batch_size}.csv', 'w', newline='')\n",
        "csv_writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "for test in tqdm(range(total_tests)):\n",
        "    # get current trial info\n",
        "    tid = test%num_trials\n",
        "    ttt = test//num_trials\n",
        "    dbs = db_sizes[ttt % len(db_sizes)]\n",
        "    ttt = ttt//len(db_sizes)\n",
        "    var = variences[ttt%len(variences)]\n",
        "\n",
        "    #reset our randomness on each new test\n",
        "    if tid == 0:\n",
        "        np.random.seed(seed)\n",
        "        seed_torch(seed)\n",
        "\n",
        "    #setup trial\n",
        "    env = gym.make(env_name, max_episode_steps=200)\n",
        "    # parameters\n",
        "    memory_size = dbs\n",
        "    target_update = 100\n",
        "    epsilon_decay = 1 / memory_size\n",
        "\n",
        "    # train\n",
        "    agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay, seed)\n",
        "    agent.train(num_frames, var, test_every, csv_writer=csv_writer)\n",
        "    csvfile.flush()\n",
        "csvfile.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uhsipra/CMPUT655_FinalProject/blob/main/CMPUT655_DQN_NSTEP_Algo_LunarLander_Config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuHO2FF5M9jM"
      },
      "source": [
        "## Configurations for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDZFH77CM9jO",
        "outputId": "c6ef3678-0c0e-42fb-ad42-f133cccc6690"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !apt install -y python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install PyVirtualDisplay==3.0\n",
        "    !pip install gymnasium==0.28.1\n",
        "    !pip install swig\n",
        "    !pip install gymnasium[box2d]\n",
        "    from pyvirtualdisplay import Display\n",
        "\n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(400, 400))\n",
        "    dis.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4sSCWUzM9jQ"
      },
      "source": [
        "# N-Step Learning\n",
        "Code based on https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/07.n_step_learning.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qxi1Lc0rM9jQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import deque\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tqdm import tqdm\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9HhwrqM9jR"
      },
      "source": [
        "## Replay buffer for N-step learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mz7raz_uM9jR"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        size: int,\n",
        "        batch_size: int = 32,\n",
        "        n_step: int = 3,\n",
        "        gamma: float = 0.99,\n",
        "        device = \"cuda\"\n",
        "    ):\n",
        "        self.obs_buf = torch.zeros([size, obs_dim], dtype=torch.float32, device=device)\n",
        "        self.next_obs_buf = torch.zeros([size, obs_dim], dtype=torch.float32, device=device)\n",
        "        self.acts_buf = torch.zeros([size], dtype=torch.int64, device=device)\n",
        "        self.rews_buf = torch.zeros([size], dtype=torch.float32, device=device)\n",
        "        self.done_buf = torch.zeros(size, dtype=torch.float32, device=device)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "        self.device = device\n",
        "\n",
        "        # for N-step Learning\n",
        "        self.n_step_buffer = deque(maxlen=n_step)\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray,\n",
        "        rew: float,\n",
        "        next_obs: np.ndarray,\n",
        "        done: bool\n",
        "    ) -> Tuple[np.ndarray, torch.tensor, float, torch.tensor, bool]:\n",
        "        transition = (obs, act, rew, next_obs, done)\n",
        "        self.n_step_buffer.append(transition)\n",
        "\n",
        "        # single step transition is not ready\n",
        "        if len(self.n_step_buffer) < self.n_step:\n",
        "            return ()\n",
        "\n",
        "        # make a n-step transition\n",
        "        rew, next_obs, done = self._get_n_step_info(\n",
        "            self.n_step_buffer, self.gamma\n",
        "        )\n",
        "        obs, act = self.n_step_buffer[0][:2]\n",
        "\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "        return self.n_step_buffer[0]\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        indices = torch.randperm(self.size, dtype=torch.int32, device=self.device)[:self.batch_size]\n",
        "\n",
        "        return dict(\n",
        "            obs=self.obs_buf[indices],\n",
        "            next_obs=self.next_obs_buf[indices],\n",
        "            acts=self.acts_buf[indices],\n",
        "            rews=self.rews_buf[indices],\n",
        "            done=self.done_buf[indices],\n",
        "            # for N-step Learning\n",
        "            indices=indices,\n",
        "        )\n",
        "\n",
        "    def sample_batch_from_idxs(\n",
        "        self, indices: np.ndarray\n",
        "    ) -> Dict[str, np.ndarray]:\n",
        "        # for N-step Learning\n",
        "        return dict(\n",
        "            obs=self.obs_buf[indices],\n",
        "            next_obs=self.next_obs_buf[indices],\n",
        "            acts=self.acts_buf[indices],\n",
        "            rews=self.rews_buf[indices],\n",
        "            done=self.done_buf[indices],\n",
        "        )\n",
        "\n",
        "    def _get_n_step_info(\n",
        "        self, n_step_buffer: Deque, gamma: float\n",
        "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
        "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
        "        # info of the last transition\n",
        "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
        "\n",
        "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
        "            r, n_o, d = transition[-3:]\n",
        "\n",
        "            rew = r + gamma * rew * (1 - d)\n",
        "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
        "\n",
        "        return rew, next_obs, done\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ele8VjNNM9jS"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "86r-7PPXM9jS"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sr2WzfBwM9jT"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"DQN Agent interacting with environment.\n",
        "\n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        memory (ReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        epsilon (float): parameter for epsilon greedy policy\n",
        "        epsilon_decay (float): step size to decrease epsilon\n",
        "        max_epsilon (float): max value of epsilon\n",
        "        min_epsilon (float): min value of epsilon\n",
        "        target_update (int): period for target model's hard update\n",
        "        gamma (float): discount factor\n",
        "        dqn (Network): model to train and select actions\n",
        "        dqn_target (Network): target model to update\n",
        "        optimizer (torch.optim): optimizer for training dqn\n",
        "        transition (list): transition information including\n",
        "                           state, action, reward, next_state, done\n",
        "        use_n_step (bool): whether to use n_step memory\n",
        "        n_step (int): step number to calculate n-step td error\n",
        "        memory_n (ReplayBuffer): n-step replay buffer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        target_update: int,\n",
        "        epsilon_decay: float,\n",
        "        seed: int,\n",
        "        max_epsilon: float = 1.0,\n",
        "        min_epsilon: float = 0.1,\n",
        "        gamma: float = 0.99,\n",
        "        # N-step Learning\n",
        "        n_step: int = 3,\n",
        "    ):\n",
        "        \"\"\"Initialization.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): openAI Gym environment\n",
        "            memory_size (int): length of memory\n",
        "            batch_size (int): batch size for sampling\n",
        "            target_update (int): period for target model's hard update\n",
        "            epsilon_decay (float): step size to decrease epsilon\n",
        "            lr (float): learning rate\n",
        "            max_epsilon (float): max value of epsilon\n",
        "            min_epsilon (float): min value of epsilon\n",
        "            gamma (float): discount factor\n",
        "            n_step (int): step number to calculate n-step td error\n",
        "        \"\"\"\n",
        "        if env.observation_space.shape is None:\n",
        "            self.ohv = True\n",
        "            self.ohv_dim = []\n",
        "            obs_dim = 0\n",
        "            for space in env.observation_space.spaces:\n",
        "                self.ohv_dim.append(obs_dim)\n",
        "                obs_dim += space.n\n",
        "            self.total_ohv_dim = obs_dim\n",
        "        elif len(env.observation_space.shape) > 0:\n",
        "            obs_dim = env.observation_space.shape[0]\n",
        "            self.ohv = False\n",
        "        else:\n",
        "            self.ohv = True\n",
        "            self.ohv_dim = [env.observation_space.n]\n",
        "            self.total_ohv_dim = env.observation_space.n\n",
        "            obs_dim = env.observation_space.n\n",
        "        action_dim = env.action_space.n\n",
        "        self.obs_dim = obs_dim\n",
        "\n",
        "        self.env = env\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = max_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.seed = seed\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        self.device=\"cpu\"\n",
        "        #print(self.device)\n",
        "\n",
        "        # memory for 1-step Learning\n",
        "        self.memory = ReplayBuffer(\n",
        "            obs_dim, memory_size, batch_size, n_step=1, gamma=gamma, device=self.device\n",
        "        )\n",
        "\n",
        "        # memory for N-step Learning\n",
        "        self.use_n_step = True if n_step > 1 else False\n",
        "        if self.use_n_step:\n",
        "            self.n_step = n_step\n",
        "            self.memory_n = ReplayBuffer(\n",
        "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma, device=self.device\n",
        "            )\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters(), lr = 1e-4)\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # epsilon greedy policy\n",
        "        if self.epsilon > np.random.random():\n",
        "            selected_action = torch.as_tensor(self.env.action_space.sample(), device=self.device).detach()\n",
        "        else:\n",
        "            selected_action = self.dqn(state).argmax().detach()\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition = [state, selected_action]\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action: np.ndarray, var) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy().item())\n",
        "        next_state = self.toOHV(next_state)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if not self.is_test:\n",
        "            #sample reward from dist\n",
        "            if var > 0:\n",
        "                reward = Normal(reward, max(0.0001, np.abs(reward)*var)).sample().to(self.device)\n",
        "\n",
        "            self.transition += [reward, next_state, done]\n",
        "\n",
        "            # N-step transition\n",
        "            if self.use_n_step:\n",
        "                one_step_transition = self.memory_n.store(*self.transition)\n",
        "            # 1-step transition\n",
        "            else:\n",
        "                one_step_transition = self.transition\n",
        "\n",
        "            # add a single step transition\n",
        "            if one_step_transition:\n",
        "                self.memory.store(*one_step_transition)\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        samples = self.memory.sample_batch()\n",
        "        indices = samples[\"indices\"]\n",
        "        loss = self._compute_dqn_loss(samples, self.gamma)\n",
        "\n",
        "        # N-step Learning loss\n",
        "        # we are gonna combine 1-step loss and n-step loss so as to\n",
        "        # prevent high-variance.\n",
        "        if self.use_n_step:\n",
        "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
        "            gamma = self.gamma ** self.n_step\n",
        "            n_loss = self._compute_dqn_loss(samples, gamma)\n",
        "            loss += n_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "    \n",
        "    def toOHV(self, state):\n",
        "        #do OHV encoding for discrete obs space\n",
        "        if self.ohv:\n",
        "            t_state = torch.zeros([self.total_ohv_dim])\n",
        "            if isinstance(state, int):\n",
        "                t_state[state] = 1\n",
        "            else:\n",
        "                for i, n in enumerate(self.ohv_dim):\n",
        "                    t_state[n+state[i]] = 1\n",
        "            state = t_state\n",
        "        return state\n",
        "\n",
        "    def train(self, num_frames: int, var: float, test_interval: int = 200, csv_writer = None):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "\n",
        "        state, _ = self.env.reset(seed=self.seed)\n",
        "        state = self.toOHV(state)\n",
        "        update_cnt = 0\n",
        "        epsilons = []\n",
        "        losses = []\n",
        "        scores = []\n",
        "        score = 0\n",
        "        elen_list = []\n",
        "        elen = 0\n",
        "\n",
        "        for frame_idx in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action, var)\n",
        "            elen+=1\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state, _ = self.env.reset(seed=self.seed)\n",
        "                state = self.toOHV(state)\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory) >= self.batch_size:\n",
        "                loss = self.update_model()\n",
        "                losses.append(loss)\n",
        "                update_cnt += 1\n",
        "\n",
        "                # linearly decrease epsilon\n",
        "                self.epsilon = max(\n",
        "                    self.min_epsilon, self.epsilon - (\n",
        "                        self.max_epsilon - self.min_epsilon\n",
        "                    ) * self.epsilon_decay\n",
        "                )\n",
        "                epsilons.append(self.epsilon)\n",
        "\n",
        "                # if hard update is needed\n",
        "                if update_cnt % self.target_update == 0:\n",
        "                    self._target_hard_update()\n",
        "\n",
        "            if (frame_idx%test_interval==0):\n",
        "                score = self.test(\"videos/n_step_learning\")\n",
        "\n",
        "                # plotting\n",
        "                if csv_writer is None and frame_idx % test_interval == 0:\n",
        "                    self._plot(frame_idx, scores, losses, epsilons)\n",
        "\n",
        "                #else writing\n",
        "                elen_list.append(score)\n",
        "                state, _ = self.env.reset(seed=self.seed)\n",
        "                state = self.toOHV(state)\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "        if csv_writer is not None:\n",
        "            csv_writer.writerow([f\"{self.memory.max_size}/{var}\", *elen_list])\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def test(self, video_folder: str, num_eps = 3) -> None:\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "\n",
        "        # for recording a video\n",
        "        naive_env = self.env\n",
        "        self.env = gym.make(naive_env.unwrapped.spec.id)\n",
        "\n",
        "        score = 0\n",
        "        eps = 0\n",
        "        with torch.inference_mode():\n",
        "            while eps < num_eps:\n",
        "                eps += 1\n",
        "                done = False\n",
        "                state, _ = self.env.reset(seed=self.seed)\n",
        "                state = self.toOHV(state)\n",
        "                while not done:\n",
        "                    action = self.select_action(state)\n",
        "                    next_state, reward, done = self.step(action, 0)\n",
        "\n",
        "                    state = next_state\n",
        "                    score += reward\n",
        "        score/=num_eps\n",
        "        self.env.close()\n",
        "\n",
        "        # reset\n",
        "        self.env = naive_env\n",
        "        self.is_test = False\n",
        "        return score\n",
        "\n",
        "    def _compute_dqn_loss(\n",
        "        self,\n",
        "        samples: Dict[str, np.ndarray],\n",
        "        gamma: float\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Return dqn loss.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        state = samples[\"obs\"]\n",
        "        next_state = samples[\"next_obs\"]\n",
        "        action = samples[\"acts\"].reshape(-1, 1)\n",
        "        reward = samples[\"rews\"].reshape(-1, 1)\n",
        "        done = samples[\"done\"].reshape(-1, 1)\n",
        "\n",
        "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
        "        #       = r                       otherwise\n",
        "        curr_q_value = self.dqn(state).gather(1, action)\n",
        "        next_q_value = self.dqn_target(next_state).max(\n",
        "            dim=1, keepdim=True\n",
        "        )[0].detach()\n",
        "        mask = 1 - done\n",
        "        target = (reward + gamma * next_q_value * mask).to(self.device)\n",
        "\n",
        "        # calculate dqn loss\n",
        "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _target_hard_update(self):\n",
        "        \"\"\"Hard update: target <- local.\"\"\"\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "    def _plot(\n",
        "        self,\n",
        "        frame_idx: int,\n",
        "        scores: List[float],\n",
        "        losses: List[float],\n",
        "        epsilons: List[float],\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        plt.subplot(131)\n",
        "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
        "        plt.plot(scores)\n",
        "        plt.subplot(132)\n",
        "        plt.title('loss')\n",
        "        plt.plot(losses)\n",
        "        plt.subplot(133)\n",
        "        plt.title('epsilons')\n",
        "        plt.plot(epsilons)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG0erORJM9jT"
      },
      "source": [
        "## Environment\n",
        "\n",
        "You can see the [code](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py) and [configurations](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L91) of CartPole-v1 from Farama Gymnasium's repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVlhzJj9M9jU"
      },
      "source": [
        "## Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c_4Bhh_rM9jU"
      },
      "outputs": [],
      "source": [
        "seed = 777\n",
        "\n",
        "def seed_torch(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.backends.cudnn.enabled:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        #torch.backends.cudnn.deterministic = True\n",
        "\n",
        "#np.random.seed(seed)\n",
        "#seed_torch(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hzmFUb3SvVU"
      },
      "source": [
        "## Run Trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je84pzKfSvVU",
        "outputId": "eafb0c95-3edc-4e0f-90bd-feead779c54c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/36 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "num_frames = 10000\n",
        "num_trials = 3\n",
        "test_every = 500\n",
        "batch_size = 64\n",
        "env_name = \"FrozenLake-v1\"\n",
        "variences = [0, 0.6, 1.2, 2]\n",
        "db_sizes = [500, 1000, 3000, 5000]\n",
        "\n",
        "\n",
        "total_tests = num_trials * len(variences) * len(db_sizes)\n",
        "\n",
        "seed = np.random.randint(0,999999)\n",
        "\n",
        "if IN_COLAB:\n",
        "    csvfile = open(f'drive/MyDrive/{env_name}-seed{seed}-batch{batch_size}.csv', 'w', newline='')\n",
        "else:\n",
        "    csvfile = open(f'./{env_name}-seed{seed}-batch{batch_size}.csv', 'w', newline='')\n",
        "csv_writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "for test in tqdm(range(total_tests)):\n",
        "    # get current trial info\n",
        "    tid = test%num_trials\n",
        "    ttt = test//num_trials\n",
        "    dbs = db_sizes[ttt % len(db_sizes)]\n",
        "    ttt = ttt//len(db_sizes)\n",
        "    var = variences[ttt%len(variences)]\n",
        "\n",
        "    #reset our randomness on each new test\n",
        "    if tid == 0:\n",
        "        np.random.seed(seed)\n",
        "        seed_torch(seed)\n",
        "\n",
        "    #setup trial\n",
        "    env = gym.make(env_name)\n",
        "    # parameters\n",
        "    memory_size = dbs\n",
        "    target_update = 100\n",
        "    epsilon_decay = 1 / memory_size\n",
        "\n",
        "    # train\n",
        "    agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay, seed)\n",
        "    agent.train(num_frames, var, test_every, csv_writer=csv_writer)\n",
        "    csvfile.flush()\n",
        "csvfile.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
